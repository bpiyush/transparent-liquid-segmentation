{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e573b-6f88-480f-b297-6e7b86fc1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee07b79-9479-47e0-8dab-53a5db3c37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, exists\n",
    "import numpy as np\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import cv2\n",
    "from natsort import natsorted\n",
    "# from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import decord\n",
    "\n",
    "import shared_utils as su\n",
    "\n",
    "from transformers import SamModel\n",
    "from finetuning_sam.lightning_models.sam import SAMLightningModule\n",
    "\n",
    "from finetuning_sam.datasets.liquid_segmentation import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6561cd6-4d3a-42f1-a296-ea8be1bedc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f938fcf-13e2-4c9a-af36-8a0f6559b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run_id = \"3y2x11qo\"\n",
    "wandb_run_dir = f\"../../audio-visual-test-of-time/{wandb_run_id}\"\n",
    "ckpt_name = \"epoch=19-step=8920.ckpt\"\n",
    "ckpt_path = join(wandb_run_dir, \"checkpoints\", ckpt_name)\n",
    "assert exists(ckpt_path)\n",
    "ckpt =  torch.load(ckpt_path)\n",
    "ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29bf126-3841-4ade-a786-5d1d496592a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM model\n",
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in sam_model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "# Load module\n",
    "module = SAMLightningModule(sam_model)\n",
    "\n",
    "# Load checkpoint\n",
    "module.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd30dd5a-f70f-43bf-8142-204624800c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "module.eval();\n",
    "module = module.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3f7de-ea94-43b7-aea9-8e06181275f1",
   "metadata": {},
   "source": [
    "### Test on validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca8f09-c81c-441a-adbf-07c4cb8f705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, processor = load_dataset(\"val\", preload=True, return_processor=True)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57cc6fc-f937-46fa-afbc-3a1ea6707406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_inference(image, prompt):\n",
    "    \"\"\"Returns visualization for inference on a single example.\"\"\"\n",
    "\n",
    "    # Create input visualization\n",
    "    show_input = su.viz.add_bbox_on_image(\n",
    "        image, prompt, color=\"yellow\",\n",
    "    )\n",
    "\n",
    "    # prepare image + box prompt for the model\n",
    "    inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = module.sam_model(**inputs, multimask_output=False)\n",
    "\n",
    "    # apply sigmoid\n",
    "    seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "    \n",
    "    # convert soft mask to hard mask\n",
    "    seg_prob = seg_prob.cpu().numpy().squeeze()\n",
    "    seg = (seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Create output visualization\n",
    "    show_output = su.viz.add_mask_on_image(\n",
    "        image,\n",
    "        mask=su.viz.alpha_mask_to_pil_image(seg)\n",
    "    )\n",
    "\n",
    "    return su.viz.concat_images([show_input, show_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e86ac-3d33-464f-b4a2-ecf78f34b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick some random indices\n",
    "indices = np.random.randint(0, len(ds), 12)\n",
    "\n",
    "# Run inference for each index\n",
    "outputs = []\n",
    "for i in su.log.tqdm_iterator(indices, desc=\"Running inference\"):\n",
    "\n",
    "    # Load item\n",
    "    item = ds.data[i]\n",
    "    \n",
    "    # Load inputs for inference\n",
    "    image = item[\"image\"]\n",
    "    prompt = su.viz.mask_to_bounding_box(item[\"cup_mask\"], perturbation=0)\n",
    "\n",
    "    # Get output\n",
    "    output = visualize_inference(image=image, prompt=prompt)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78bbd0-8fa3-4239-b274-b05350e6ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "su.viz.show_grid_of_images(\n",
    "    outputs, n_cols=4, figsize=(4 * 4, 2 * 3), subtitles=indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5e4fe-d486-4be8-87fd-58a3a21ccc59",
   "metadata": {},
   "source": [
    "### Test on samples from `PouringIROS2019`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf7401-740e-40aa-8bfe-eeb930222d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/ssd/pbagad/datasets/\"\n",
    "data_dir = join(data_root, \"PouringIROS2019\")\n",
    "video_dir = join(data_dir, \"resized_data\")\n",
    "annot_dir = join(data_dir, \"annotations\")\n",
    "\n",
    "# Load bounding box annotations\n",
    "annot_path = join(annot_dir, \"water_container_detections-v1.pt\")\n",
    "assert exists(annot_path)\n",
    "annotations = torch.load(annot_path)\n",
    "\n",
    "# Load data\n",
    "csv_path = join(data_dir, \"metadata/all_liquids_in_transparent_containers.csv\")\n",
    "assert exists(csv_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Add video path\n",
    "row_id = \"video_id\"\n",
    "def _get_video_path(row):\n",
    "    \"\"\"Returns the path to the video file.\"\"\"\n",
    "    video_path = join(video_dir, row[row_id] + \".mp4\")\n",
    "    assert exists(video_path), \"video_path does not exist.\"\n",
    "    return video_path\n",
    "df[\"video_path\"] = df.apply(_get_video_path, axis=1)\n",
    "df = df[df[\"video_path\"].apply(os.path.exists)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791d458-f9d8-46c9-a4ef-54f3430f6b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames_from_a_video(i, n_frames=12, imsize=256, crop=True):\n",
    "    row = df.iloc[i].to_dict()\n",
    "\n",
    "    video_id = row[\"video_id\"]\n",
    "    video_path = row[\"video_path\"]\n",
    "    vr = decord.VideoReader(video_path)\n",
    "    \n",
    "    frame_indices = np.linspace(0, len(vr) - 1, n_frames, dtype=int)\n",
    "    frames = vr.get_batch(frame_indices).asnumpy()\n",
    "    frames = [PIL.Image.fromarray(f) for f in frames]\n",
    "\n",
    "    # Crop frames (only if annotations are available)\n",
    "    if (video_id in annotations) and crop:\n",
    "        box = annotations[video_id]\n",
    "        frames = [f.crop(list(box)) for f in frames]\n",
    "\n",
    "    # Resize for SAM compatibility\n",
    "    frames = [f.resize((imsize, imsize)) for f in frames]\n",
    "\n",
    "    # Define prompts (entire size of the image since we already cropped)\n",
    "    prompts = [[0, 0, imsize, imsize] for _ in range(len(frames))]\n",
    "\n",
    "    return frames, prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6be66-edc5-4619-bd99-c58029840b9c",
   "metadata": {},
   "source": [
    "**Samples from the same video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d38d3-b6f7-4dcd-9185-48334807c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, prompts = load_frames_from_a_video(130)\n",
    "su.viz.show_grid_of_images(frames, n_cols=len(frames), figsize=(len(frames) * 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f29bd7-5f86-4d55-9e98-5b972c88977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference for each index\n",
    "outputs = []\n",
    "for image, prompt in zip(frames, prompts):\n",
    "    outputs.append(visualize_inference(image=image, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac5d69-2860-46d5-87e5-cf7fd392afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "su.viz.show_grid_of_images(\n",
    "    outputs, n_cols=4, figsize=(4 * 4, 2 * 3), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7356a-824f-4878-b62d-84bec66220b1",
   "metadata": {},
   "source": [
    "Can we use our idea to get better liquid segmentations? Like even if we get output right at time $t$, ideally, it should translate to excellent outputs throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf9caf-5ed7-4534-b13f-eb5ee964c7c8",
   "metadata": {},
   "source": [
    "### Test on YouTube videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed437fc-5e15-4b13-b3d2-c65a94d482a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(data_root, \"Viscaural/v25\")\n",
    "video_dir = os.path.join(data_dir, \"clips\")\n",
    "annot_dir = os.path.join(data_dir, \"annotations\")\n",
    "\n",
    "# Load data\n",
    "csv_path = os.path.join(\n",
    "    data_dir,\n",
    "    \"splits/download_2023-05-21_11-20-59-sliding_predictions_nms_top2000-clean315-clean55.csv\",\n",
    ")\n",
    "assert exists(csv_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load annotations\n",
    "annot_path = os.path.join(\n",
    "    data_dir,\n",
    "    \"annotations/water_glass_detections-first_frame-v1-0-311.pt\",\n",
    ")\n",
    "assert exists(annot_path)\n",
    "annotations = torch.load(annot_path)\n",
    "\n",
    "# Add video path\n",
    "row_id = \"item_id\"\n",
    "def _get_video_path(row):\n",
    "    \"\"\"Returns the path to the video file.\"\"\"\n",
    "    video_path = join(video_dir, row[row_id] + \".mp4\")\n",
    "    assert exists(video_path), \"video_path does not exist.\"\n",
    "    return video_path\n",
    "df[\"video_path\"] = df.apply(_get_video_path, axis=1)\n",
    "df = df[df[\"video_path\"].apply(os.path.exists)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b59a0-6a14-4d0a-bef3-1c447b1f0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames_from_a_video(i, n_frames=12, imsize=256, crop=True):\n",
    "    row = df.iloc[i].to_dict()\n",
    "\n",
    "    video_id = row[\"item_id\"]\n",
    "    video_path = row[\"video_path\"]\n",
    "    vr = decord.VideoReader(video_path)\n",
    "    \n",
    "    frame_indices = np.linspace(0, len(vr) - 1, n_frames, dtype=int)\n",
    "    frames = vr.get_batch(frame_indices).asnumpy()\n",
    "    frames = [PIL.Image.fromarray(f) for f in frames]\n",
    "\n",
    "    # Crop frames (only if annotations are available)\n",
    "    if (video_id in annotations) and crop:\n",
    "        box = annotations[video_id]\n",
    "        box = [v / 2. for v in box]\n",
    "        frames = [f.crop(list(box)) for f in frames]\n",
    "\n",
    "    # Resize for SAM compatibility\n",
    "    frames = [f.resize((imsize, imsize)) for f in frames]\n",
    "\n",
    "    # Define prompts (entire size of the image since we already cropped)\n",
    "    prompts = [[0, 0, imsize, imsize] for _ in range(len(frames))]\n",
    "\n",
    "    return frames, prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb0639-d6d9-4a28-a20e-0827abe7ef16",
   "metadata": {},
   "source": [
    "**Samples from the same video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb8cc6-1000-465c-94ba-2e378cc45cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, prompts = load_frames_from_a_video(8)\n",
    "su.viz.show_grid_of_images(frames, n_cols=len(frames), figsize=(len(frames) * 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d670b6-eb55-40b2-ab8d-ec1b057c947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference for each index\n",
    "outputs = []\n",
    "for image, prompt in zip(frames, prompts):\n",
    "    outputs.append(visualize_inference(image=image, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a44bcd-f441-45d7-8997-3b87780ff2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "su.viz.show_grid_of_images(\n",
    "    outputs, n_cols=4, figsize=(4 * 4, 2 * 3), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c01c2-b2cf-4cba-b3b6-e1330b9c5dd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Legacy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace4471-94cb-4bf3-9ba7-876c797b2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select index \n",
    "i = 0\n",
    "item = ds.data[i]\n",
    "\n",
    "# Load inputs for inference\n",
    "image = item[\"image\"]\n",
    "prompt = su.viz.mask_to_bounding_box(item[\"cup_mask\"], perturbation=0)\n",
    "show_input = su.viz.add_bbox_on_image(\n",
    "    image, prompt, color=\"yellow\",\n",
    ")\n",
    "show_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915ea7d-58bf-44d9-8344-a2eb9adf6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image + box prompt for the model\n",
    "inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d0310-c3c6-4862-aca9-2986ee846003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = module.sam_model(**inputs, multimask_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb5d9e-6a90-4f2d-94cb-951bf9db81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply sigmoid\n",
    "seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "\n",
    "# convert soft mask to hard mask\n",
    "seg_prob = seg_prob.cpu().numpy().squeeze()\n",
    "seg = (seg_prob > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac24ce36-269f-4ebc-a781-52b5cac8d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_output = su.viz.add_mask_on_image(\n",
    "    image,\n",
    "    mask=su.viz.alpha_mask_to_pil_image(seg)\n",
    ")\n",
    "su.viz.concat_images([show_input, show_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b08808-b9d6-44a3-b6c0-3fac9ea1209a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
